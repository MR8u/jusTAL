{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from pathlib import Path\n",
    "from src.preprocess import fix_text\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import re\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"fr_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_xml(xml_code):\n",
    "    tokens = []\n",
    "    root = ET.fromstring(xml_code)\n",
    "    _tokenize_element(root, tokens)\n",
    "    return tokens\n",
    "\n",
    "def _tokenize_element(element, tokens):\n",
    "    tag = \"<\" + element.tag + (\" \" + ' '.join(f'{attr}=\"{value}\"' for attr, value in element.attrib.items()) if len(element.attrib)>0 else '') + \">\"\n",
    "    tokens.append(tag)  # Start tag\n",
    "    if element.text and element.text.strip():\n",
    "        tokens.extend(el.text for el in nlp(element.text.strip())) # String value\n",
    "    for child in element:\n",
    "        if child.tag == ET.Comment:\n",
    "            pass  # Comment\n",
    "        else:\n",
    "            _tokenize_element(child, tokens)  # Recursive call for child elements\n",
    "    tokens.append(\"</\" + element.tag + \">\")  # End tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = Path('.data/text_tests/')\n",
    "# PATH = Path('D:/Users/subje/Documents/GitHub/jusTAL/.data/text_termines')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "for path in PATH.glob('*.xml'):\n",
    "    for disc in BeautifulSoup(fix_text(path.read_text(encoding='utf-8')), features='xml').findAll(attrs={'type':'discussion'}):\n",
    "        texts.append((disc, path.stem))\n",
    "    # texts.append((BeautifulSoup(path.read_text(encoding='utf-8'), features='xml').find('body').text, path.stem))\n",
    "df = pd.DataFrame(texts, columns=['text', 'id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'value_ner' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 29\u001b[0m\n\u001b[0;32m     27\u001b[0m     relation \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mNULL\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     28\u001b[0m     spk_trgt \u001b[39m=\u001b[39m cur_speaker\n\u001b[1;32m---> 29\u001b[0m tab\u001b[39m.\u001b[39mappend(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\r\u001b[39;00m\u001b[39m{\u001b[39;00mm\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m-\u001b[39m\u001b[39m{\u001b[39;00mn\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m{\u001b[39;00mlen_words\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m-\u001b[39m\u001b[39m{\u001b[39;00mlen_words\u001b[39m+\u001b[39m\u001b[39mlen\u001b[39m(token)\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m{\u001b[39;00mtoken\u001b[39m}\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m{\u001b[39;00m\u001b[39mstr\u001b[39m(value_ner)\u001b[39m}\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m{\u001b[39;00mrelation\u001b[39m}\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m{\u001b[39;00mspk_trgt\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m     30\u001b[0m len_words\u001b[39m+\u001b[39m\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(token)\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'value_ner' is not defined"
     ]
    }
   ],
   "source": [
    "tab = []\n",
    "list_tokens = []\n",
    "len_words = -1\n",
    "for text in df.text:\n",
    "    tokens_init = tokenize_xml(str(text))\n",
    "    i=0\n",
    "    # print(f'#Text={\" \".join(tokens)}')\n",
    "    list_cpt = 0\n",
    "    for c, el in enumerate(tokens_init): #divide into lists of tokens by </u>\n",
    "        if el=='</u>':\n",
    "            list_tokens.append(tokens_init[list_cpt:c+1])\n",
    "            list_cpt = c+1\n",
    "\n",
    "for m, tokens in enumerate(list_tokens):\n",
    "    tab.append(f'\\r\\r#Text={\" \".join(tokens)}')\n",
    "    cur_speaker = '_'\n",
    "    for n, token in enumerate(tokens):\n",
    "        # print(f'{m+1}-{n+1}\\t{i}-{i+len(token)}\\t{token}')\n",
    "        value_ner = relation = spk_trgt = '_'\n",
    "        if re.search('<u who=', token):\n",
    "            # print(re.findall('who=\"#(.*?)\"', token))\n",
    "            value_ner = True\n",
    "            cur_speaker = f'{m+1}-{n+1}'\n",
    "        if re.search('<span ana', token):\n",
    "            # print(re.findall('ana=\"#(.*?)\"', token))\n",
    "            value_ner = True\n",
    "            relation = 'NULL'\n",
    "            spk_trgt = cur_speaker\n",
    "        tab.append(f'\\r{m+1}-{n+1}\\t{len_words+1}-{len_words+len(token)+1}\\t{token}\\t{str(value_ner)}\\t{relation}\\t{spk_trgt}')\n",
    "        len_words+=len(token)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tab = []\n",
    "# len_words = -1\n",
    "# for m, text in enumerate(df.text):\n",
    "#     tokens = tokenize_xml(str(text))\n",
    "#     i=0\n",
    "#     # print(f'#Text={\" \".join(tokens)}')\n",
    "#     tab.append(f'\\r\\r#Text={\" \".join(tokens)}')\n",
    "#     cur_speaker = '_'\n",
    "#     for n, token in enumerate(tokens):\n",
    "#         # print(f'{m+1}-{n+1}\\t{i}-{i+len(token)}\\t{token}')\n",
    "#         speaker = target = value = relation = spk_trgt = '_'\n",
    "#         if re.search('<u who=', token):\n",
    "#             # print(re.findall('who=\"#(.*?)\"', token))\n",
    "#             speaker = True\n",
    "#             value = 'PERSON'\n",
    "#             cur_speaker = f'{m+1}-{n+1}'\n",
    "#         if re.search('<span ana', token):\n",
    "#             # print(re.findall('ana=\"#(.*?)\"', token))\n",
    "#             target = True\n",
    "#             value = 'PERSON'\n",
    "#             relation = 'NULL'\n",
    "#             spk_trgt = cur_speaker\n",
    "#         tab.append(f'\\r{m+1}-{n+1}\\t{len_words+1}-{len_words+len(token)+1}\\t{token}\\t{str(speaker)}\\t{str(target)}\\t{value}\\t{relation}\\t{spk_trgt}')\n",
    "#         len_words+=len(token)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test.tsv', 'w') as f:\n",
    "    f.write('#FORMAT=WebAnno TSV 3.3\\r')\n",
    "    f.write('#T_SP=webanno.custom.JusTALNER|value_ner\\r')\n",
    "    f.write('#T_RL=webanno.custom.JusTALREL|value_rel|BT_webanno.custom.JusTALNER\\r')\n",
    "    for line in tab:\n",
    "            f.write(line)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "justal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
